{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f21d5059-73b7-456c-82ea-f1a605a0ad14",
   "metadata": {
    "tags": []
   },
   "source": [
    "# High-performance computing applied in AI applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad37b173-ed95-46f5-bdf0-0bdf3016caed",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>\n",
    "High-performance computing, known as HPC, is a field of modern computing whose goal is to solve computational problems of high complexity and large volumes of data by operating by dividing complex problems into smaller parts that are processed simultaneously by several processors, accelerating the resolution time. HPC enables scientists, engineers, and researchers to perform highly detailed simulations, massive data analysis, and precise modeling that would be impractical or unachievable using conventional systems.\n",
    "</p>\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "HPC systems are designed to handle large volumes of data and perform intensive calculations in a fraction of the time it would take on conventional computers. An HPC comprises one or several supercomputers of interconnected high-performance processors, large amounts of memory, and fast storage to handle intensive workloads.\n",
    "</p>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "<img src=\"./images/figure01_ponte_vecchio.jpg\" style=\"width: 500px;\">\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b762f150-1969-42e6-b8ce-dbb31fbebb96",
   "metadata": {},
   "source": [
    "## ⊗ **Why use parallel computing?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff0cc88-a7de-4cb6-8b31-40f3d1de4692",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>   \n",
    "Parallel computing is widely used in many areas such as scientific simulations, graphics rendering, big data analysis, machine learning, artificial intelligence, image processing and many more. There are several approaches to implementing parallel computing, of which we can include data parallelism, task parallelism, instruction parallelism, bit-level parallelism, thread-level parallelism, among others.\n",
    "</p>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "<img src=\"./images/figure02_parallel_computing.png\" style=\"width: 1000px;\">\n",
    "</div>\n",
    "\n",
    "<p style='text-align: justify;'>   \n",
    "One of the main gains parallel computing provides is the remarkable performance acceleration. You can get more work done in less time by running multiple tasks simultaneously. This aspect is particularly advantageous for solving complex problems that often involve intensive calculations or the analysis of vast data sets.\n",
    "Parallel computing is essential for dealing with the growing data generated in our digital world. In machine learning and artificial intelligence, training complex models in parallel is critical for creating effective AI systems in areas such as pattern recognition, natural language processing, and computer vision.\n",
    "</p>\n",
    "\n",
    "<p style='text-align: justify;'> \n",
    "HPC and parallel computing can be used in several scenarios. Let's meet some of them below.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a215da7-040a-4598-8772-bf1e990001fa",
   "metadata": {},
   "source": [
    "## ⊗ **HPC applied in AI**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22840e1e-0850-4e66-90db-d1f795bb6ea6",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'> \n",
    "HPC plays a key role when it comes to applications using Artificial Intelligence, since a large computational power is needed to be able to train increasingly complex AI models and perform analyzes on massive data sets.\n",
    "</p>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "<img src=\"./images/figure03_aurora_supercomputing.jpg\" style=\"width: 500px;\">\n",
    "</div>\n",
    "    \n",
    "<p style='text-align: justify;'> \n",
    "A notable example is Intel's Aurora supercomputer, which plays a key role in research areas as diverse as neuroscience, aerospace simulation, universe exploration, and artificial intelligence. These surveys require an extremely high processing capacity,\n",
    "making the application of structures such as HPC essential. Conducting research in this direction requires the use of computational algorithms capable of dealing with large volumes of data and that also have resources for implementing artificial intelligence solutions. For example, in neuroscience research,\n",
    "Aurora can help simulate complex neural networks and analyze brain data at scale, leading to advances in understanding neurological disorders and developing more effective treatments.\n",
    "</p>\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "In aerospace simulations and exploration of the universe, Aurora allows the modeling of complex phenomena, such as the behavior of planetary systems and aircraft flight dynamics, contributing to space exploration and the development of more advanced technologies. In summary, the intersection between HPC and life science research, space and AI drives scientific discoveries and technological advances that have the potential to transform our lives and our understanding of the world around us.\n",
    "</p>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2d0c4c-17a0-481f-928b-cc669f30c1ec",
   "metadata": {},
   "source": [
    "## ⊗ **HPC uses cases**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c833036e-082b-4f1d-88f7-05ffa4221acd",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'> \n",
    "HPC is often used in fields where processing requirements are extraordinarily high and exceed the capabilities of conventional computer systems. Here are some examples of HPC use cases:\n",
    "</p>\n",
    "    \n",
    "<div style=\"text-align:center\">\n",
    "<img src=\"./images/figure04_hpc_applications.png\" style=\"width: 500px;\">\n",
    "</div>\n",
    "\n",
    "* **Machine learning and artificial intelligence:** training complex machine learning models requires a lot of computing power. HPC allows you to train models faster and handle larger datasets, resulting in advances in AI, pattern recognition and data analysis;\n",
    "\n",
    "* **Biomedical research:** HPC accelerates the virtual screening of molecules, assessing how they interact with target proteins. This streamlines the drug discovery process, saving time and resources;\n",
    "\n",
    "* **Aerodynamics and flight simulation:** the aerospace industry uses HPC to simulate the behavior of aircraft,improve wing design, optimize fuel efficiency and study aerodynamics;\n",
    "\n",
    "* **Exploration of natural resources and petroleum:** the simulation of oil and gas reservoirs, as well as the exploration of mineral resources, require complex models and intensive calculations. HPC helps make informed decisions about locating and exploiting these resources;\n",
    "\n",
    "* **Particle physics:** particle physics research requires HPC to analyze data generated by particle accelerators such as the LHC (Large Hadron Collider);\n",
    "\n",
    "* **Scientific research and simulations:** HPC allows the modeling of natural phenomena and processes that would be almost impossible to observe experimentally. For example, simulating particle interactions in a particle accelerator or simulating long-term weather processes.\n",
    "\n",
    "<p style='text-align: justify;'> \n",
    "These are just a few examples of the many use cases for HPC. In general, it plays a crucial role in areas that need advanced computational capacity to solve complex problems, often driving innovation and scientific and technological progress.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c549e24-b460-4f22-8cea-0b32287d71d4",
   "metadata": {},
   "source": [
    "## **HPC as solutions for AI**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e4518c-36e7-4d4b-a3d1-aa1a62981587",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'> \n",
    "When we want to deal with a large volume of information in artificial intelligence applications, aiming to substantially reduce the time required to solve the problem, it is essential that specific software tools are implemented for this purpose. Let's now explore two of the most prominent libraries: <b>TensorFlow</b> and <b>PyTorch</b>. These tools play a central role in creating and training AI models in processing-intensive environments, let's get to know each one of them.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab72b0a4-8133-45ea-9acf-a5df54845269",
   "metadata": {},
   "source": [
    "### ⊗ **Tensorflow**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf342a08-64df-4712-9943-25d9a01f3173",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>\n",
    "<a href='https://www.tensorflow.org/api_docs' target='_blank'><em>Tensorflow</em></a> is an open-source library focused on high-performance numerical computing, especially suitable for training and deploying machine learning and deep learning models. It is used in various applications, from computer vision to natural language processing.\n",
    "</p>\n",
    "<p style='text-align: justify;'>\n",
    "In our algorithms, we will be using a package called Keras, which is nothing more than a high-level API for building and training neural networks. Its main feature is to simplify and streamline the development of deep learning models.\n",
    "</p>  \n",
    "<p style='text-align: justify;'>\n",
    "Let's see how we can access and utilize our GPU to train a simple neural network using TensorFlow.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d194d0-9328-435f-abaf-4746de85e2c7",
   "metadata": {},
   "source": [
    "#### **Checking GPU availability**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27db308b-c108-489d-aef4-17dcc9ead7e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available. Using CPU.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check for available GPUs\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    # Configure GPU memory allocation dynamically\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "    # Display information about available GPU\n",
    "    for i, gpu in enumerate(gpus):\n",
    "        print(f\"GPU {i + 1}: {gpu.name}\")\n",
    "else:\n",
    "    print(\"No GPU available. Using CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93378488-1958-4169-8851-9634c3fa9246",
   "metadata": {},
   "source": [
    "####  **Creating a neural network with Tensorflow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c513f92-82c8-4db9-8d0f-86f0ebceadc7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 0.7790 - accuracy: 0.7500\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7787 - accuracy: 0.7500\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7783 - accuracy: 0.7500\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7780 - accuracy: 0.7500\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7776 - accuracy: 0.7500\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7773 - accuracy: 0.7500\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7769 - accuracy: 0.7500\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7766 - accuracy: 0.7500\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7762 - accuracy: 0.7500\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7759 - accuracy: 0.7500\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7756 - accuracy: 0.7500\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7752 - accuracy: 0.7500\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7749 - accuracy: 0.7500\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.7745 - accuracy: 0.7500\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7742 - accuracy: 0.7500\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7738 - accuracy: 0.7500\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7735 - accuracy: 0.7500\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7732 - accuracy: 0.7500\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7728 - accuracy: 0.7500\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7725 - accuracy: 0.7500\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.7722 - accuracy: 0.7500\n",
      "Model accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "# Set the training data\n",
    "X_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_train = np.array([0, 1, 1, 0])\n",
    "\n",
    "# create the model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(units=1, input_dim=2, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the GPU\n",
    "with tf.device('/GPU:0'):\n",
    "    model.fit(X_train, y_train, epochs=20)\n",
    "\n",
    "# rate the model\n",
    "accuracy = model.evaluate(X_train, y_train)[1]\n",
    "print(f'Model accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b0713a-ae86-40ff-b8b5-67c6584b643d",
   "metadata": {},
   "source": [
    "### ⊗ **Pytorch**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2bcf97-c2cb-486f-8f26-def566ae4b9b",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'> \n",
    "<a href='https://pytorch.org/docs/stable/index.html' target='_blank'><em>Pytorch</em></a> is an open source machine learning library known for its flexibility and ease of use, making it a popular choice among deep learning researchers and developers. Let's see the same code we did in tensorflow only now with Pytorch.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59402a2-4733-47b2-ae15-df9a1a870e07",
   "metadata": {},
   "source": [
    "####  **Checking GPU availability**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c189b174-bee5-4270-b3cf-3ed4d90411df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available. Using CPU.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of available GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "\n",
    "    # Display information about available GPUs\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"No GPU available. Using CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2972da-3852-4a28-b8f4-34a593cd5bf9",
   "metadata": {
    "tags": []
   },
   "source": [
    "####  **Creating a neural network with Pytorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "baefb205-d9ab-4d27-8ed0-07e5ae3d4582",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Define training data as tensors\n",
    "X_train = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
    "y_train = torch.tensor([0, 1, 1, 0], dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# create the model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# Moving model and data to GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# train the model\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# rate the model\n",
    "with torch.no_grad():\n",
    "    predicted = model(X_train)\n",
    "    predicted = (predicted > 0.5).float()\n",
    "    accuracy = (predicted == y_train).sum().item() / len(y_train)\n",
    "    print(f'Model accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef8f158-6d94-447b-844e-71e3307be024",
   "metadata": {
    "id": "BMvo8TG2WWZE"
   },
   "source": [
    "##  ☆ Challenge: Zoo breakout!☆ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52bfc0f-feac-431e-8365-7ac4dfbb0135",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'> \n",
    "    Recently, there was an unexpected incident at the local zoo, <b>Orange Grove Zoo</b>: all the animals escaped from their enclosures and are now roaming freely in the zoo. To deal with this situation, we need your help to locate and classify the escaped animals, distinguishing each animal class and identifying possible vehicles that may be in the same environment.\n",
    "</p>\n",
    "<p style='text-align: justify;'> \n",
    "You have been assigned as the person responsible for developing a computer vision system capable of identifying and classifying the escaped animals, as well as identifying the presence of vehicles in the images. For this challenge, we will use the CIFAR-10 dataset and the tensorflow library to train a deep learning model.\n",
    "</p>\n",
    "CIFAR-10  datasets provide a comprehensive collection of 32x32 pixel images, grouped into 10 distinct classes.\n",
    "\n",
    "- [CIFAR-10 Dataset](https://www.cs.toronto.edu/~kriz/cifar.html): CIFAR-10 consists of 60,000 images, each belonging to one of the ten classes: airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. This dataset offers a diverse set of images representing everyday objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0af1c08-6dbd-4edc-bd08-10d0b343e9c8",
   "metadata": {
    "id": "X9pJ2tDOCaN6"
   },
   "source": [
    "a) **Create** deep neural network model utilizing the tensorflow library for the classification of animals and vehicles on a CPU using CIFAR-10 dataset,\n",
    "\n",
    "b) **Measure** the algorithm execution time for CIFAR-10 with CPU.\n",
    "\n",
    "c) **Justify** why it is more interesting to use tools like tensorflow and pytorch in conjunction with a GPU instead of a CPU?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78eb535f-68bb-4703-bb0a-f74372dbd09c",
   "metadata": {
    "id": "8_Ch1ZPO1bIC"
   },
   "source": [
    "### ☆ Solution ☆"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81175015-e630-416f-9d36-2a30cc9830da",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### ⊗ Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a211df6a-b172-45b0-8375-8a1c24add541",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b915680-f10b-4f3a-a799-7c09a243a7f8",
   "metadata": {},
   "source": [
    "#### ⊗ Verify the devices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c257254-96bc-4592-b247-e2f8b2f67009",
   "metadata": {},
   "source": [
    "It is very important, before trying to execute anything on any device, to verify if it is available and if pytorch can use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bd569a7-3acb-4caa-86d1-f2ead2ea05b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29138cf0-ba87-4c6a-8c88-111dc159b833",
   "metadata": {},
   "source": [
    "#### ⊗ Transformations to the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2915d780-1d61-4f6a-9ec4-154e64ac4896",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'> \n",
    "    As part of the data preparation process, we create a <b>transforms</b> object to apply specific transformations to the data. These transformations are commonly used in training datasets to enhance data diversity and ready images for utilization in a deep learning model, such as a Convolutional Neural Network (CNN).\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "210c3993-fedc-4018-88b8-48dc30df64da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131d6960-4abe-48b3-9167-3bd9454e037d",
   "metadata": {},
   "source": [
    "#### ⊗ Downloading the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc889d5f-3ace-4c72-bf7c-5ec768c8bcc1",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'> \n",
    "Following that, download the CIFAR-10 dataset and load it into the code. Define the neural network as we have done in previous notebooks, and remember to move this network instance to the previously defined device.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89db0f4-fd04-4b3e-a59e-79220d531953",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be0375d-d0f8-41f0-9530-5459d2c125f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### ⊗ Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0c9319-c3a3-4895-9c60-58ae53e2f6ff",
   "metadata": {},
   "source": [
    "Now it is necessary to create the model for our neural network using pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9295417d-ad2f-421c-b053-16f5b4db5b5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=8192, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 128 * 8 * 8)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb622a0f-9d02-4d7c-aaf5-572e4529ebed",
   "metadata": {},
   "source": [
    "#### ⊗ Training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d4fa56-591a-418b-b52e-ff58d543565c",
   "metadata": {},
   "source": [
    "Now the training of our neural network will be carried out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be67ba50-0e8d-435f-ada7-b0a98b91aa8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.4932993039908007\n",
      "Epoch 2, Loss: 0.47943620257975195\n",
      "Epoch 3, Loss: 0.47039098767063503\n",
      "Epoch 4, Loss: 0.4571740362802735\n",
      "Epoch 5, Loss: 0.4399987073505626\n",
      "Epoch 6, Loss: 0.42618661684453335\n",
      "Epoch 7, Loss: 0.4213465188470338\n",
      "Epoch 8, Loss: 0.41050467756398196\n",
      "Epoch 9, Loss: 0.40083302690854766\n",
      "Epoch 10, Loss: 0.3957774586918409\n",
      "\n",
      "CPU Training time: 811.17 seconds or (13.52 minutes)\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "net.to(device)\n",
    "\n",
    "cpu_start_time = time.time()\n",
    "\n",
    "for epoch in range(10):  \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch+1}, Loss: {running_loss / len(trainloader)}')\n",
    "\n",
    "cpu_end_time = time.time()\n",
    "\n",
    "cpu_time = cpu_end_time - cpu_start_time\n",
    "\n",
    "print(f\"\\nCPU Training time: {cpu_time:.2f} seconds or ({cpu_time / 60:.2f} minutes)\")\n",
    "\n",
    "torch.save(net.state_dict(), 'cifar10_cpu_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a376aa-7b95-4052-9200-3b1fa2210c3e",
   "metadata": {},
   "source": [
    "#### ⊗ Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e0e59c-7348-4a16-aa20-0b971c582b7f",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'> \n",
    "You may have noticed that we completed our training with only 10 epochs, and it took around <b>13.52 minutes!!!</b> That means it's a reasonably long time for a small number of epochs. Now, imagine if we increased it to 100 epochs or used a larger dataset like CIFAR-100! It would become impractical to perform this kind of task on conventional computing resources, such as a laptop with a CPU, for example. Therefore, it is necessary to rely on much greater computational power, which is provided by supercomputers and GPUs.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f1cb8f-ab67-49b8-96e5-69802ce692b1",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bdd67f-de47-4431-9da4-2471925c830e",
   "metadata": {
    "tags": []
   },
   "source": [
    "<p style='text-align: justify;'>\n",
    "In this notebook we have shown: \n",
    "\n",
    "- The definitios about HPC applied in AI applications,\n",
    "- Parallel computing concepts,\n",
    "- Some HPC uses cases, and solutions for AI using Tensorflow and Pytorch,\n",
    "- An example of training a neural network on the CPU using pytorch and CIFAR-10.\n",
    "</p>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4cbf28-b270-4cc3-b8dd-d6e8f9962c5e",
   "metadata": {},
   "source": [
    "## Clear the memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ff4e3e-b023-44b5-8b0f-91c6553bed72",
   "metadata": {},
   "source": [
    "Before moving on, please execute the following cell to clear up the CPU memory. This is required to move on to the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c73882cd-c157-4608-83fc-07491c5a8223",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import IPython\n",
    "#app = IPython.Application.instance()\n",
    "#app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c8df02-d305-49f3-a5f4-8120c990b27f",
   "metadata": {},
   "source": [
    "## Next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1317d6bd-a969-407e-a991-cd38f5861c3e",
   "metadata": {},
   "source": [
    "In this section you learned the meaning of HPC applied in AI applications, and how we can use the processing speed of a GPU to improve the performance of our artificial intelligence algorithms. In the next notebook we will study HPC as solutions for AI using the tool call [_02-hpc-simulations-tensorflow.ipynb_](02-hpc-simulations-tensorflow.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
