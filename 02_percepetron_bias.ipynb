{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icd8YEjb0k3Z"
      },
      "source": [
        "# Applying Bias parameter in perceptron using Intel® Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZYWbeOt-2Zn"
      },
      "source": [
        "In this section, we will add a BIAS parameter to the Perceptron Algorithm that we built in the previous notebooks using Intel® Python. Without a bias, the activation function of the perceptron would be forced to pass through the origin (0,0), which can limit its ability to learn patterns in more complex data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0cQveSt-6DL"
      },
      "source": [
        "## Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXDHeyVl-4KB"
      },
      "source": [
        "* **Understand** the concept of BIAS parameter;\n",
        "* **Learn** how to implement BIAS parameter;\n",
        "* **Adjust** BIAS weights to get more precise results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AY2d3T6i-_XA"
      },
      "source": [
        "## Perceptron Theory: Linearly separable problems.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hq2Fah8mBK1d"
      },
      "source": [
        "The perceptron is a learning model that can only solve problems that can be divided into linear segments. This indicates that it can only separate and categorize data that can be divided by a single line or hyperplane.\n",
        "\n",
        "For cases where data can be divided into two or more different classes and separated by a line or hyperplane in the feature space, the perceptron is appropriate, in other words.\n",
        "\n",
        "This dynamic is well represented at Figure 1.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Figure1](./images/figure1_perceptron_bias.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcQUPZyWCDG9"
      },
      "source": [
        "Figure 1 - Linear and Non Linear Separable Problems Examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "De95n7F5DQMO"
      },
      "source": [
        "From the point of view of a problem with two parameters, when the classification value depends on \"AND\" and \"OR\" relationships between inputs, it means that it is a linearly separable problem. In cases where the relationship is \"XNOR\", it is a non-linearly separable problem, as exemplified in figure 2."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Figure2](./images/figure2_perceptron_bias.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JF53Hb5DS1z"
      },
      "source": [
        "Figure 2 - \"AND\", \"OR\", \"XNOR\" relations at two Parameters Linear separable problems.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6p2zUxw0HU1"
      },
      "source": [
        "## The Problem: The separation line \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PE5N3omDsya"
      },
      "source": [
        "The last Perceptron Algorithms classifies data into two classes, to do that they used separation lines that were forced to pass through the origin (0,0). That characteristic limits the model's ability to adjust the line correctly, reducing the accuracy of predictions.\n",
        "\n",
        "Looking at figure 3, it shows a case of a linearly separable problem where it is not possible to draw the separation line in a way that it passes through the origin. Therefore, it would not be possible to use the models worked on so far to solve this problem."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Figure3](./images/figure3_perceptron_bias.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ks42-a6OFkCb"
      },
      "source": [
        "Figure 3 - Linearly separable problem that can't be separeted by a line that pass through the origin."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdKQjrchGbLn"
      },
      "source": [
        "## The Solution: BIAS parameter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqPOJJmeGdcK"
      },
      "source": [
        "The word \"bias\" in the context of perceptrons refers to a further parameter that is added to the model's input and acts as an offset or threshold for activation. By moving the decision boundary further from the origin, as illustrated in figure 4, the perceptron is able to segregate data at some specific cases that are linearly separable.\n",
        "\n",
        "For the best possible prediction accuracy, it can be changed during model training. In other words, the bias allows the model to be more flexible to the data it is trying to learn from and forecast, in addition to enabling the resolution of more specific linear issues."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Figure4](./images/figure4_perceptron_bias.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35hyP80QHsxl"
      },
      "source": [
        "Figure 4 - Perceptron with BIAS parameter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVKHPTtIH9V9"
      },
      "source": [
        "## Implementing BIAS parameter at Perceptron Neural Network with Intel® Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Idl-dYqJepN"
      },
      "source": [
        "Taking the previous challenge about Roses and Violets, we just need to add one more constant parameter to the inputs, which will be our BIAS. Thus, during the training period of the perceptron weights, it will adjust to the correct height for each problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wb0PGYaqKBZV",
        "outputId": "fbb170a4-3d89-49ec-e48f-df834c25cd52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.9999999999997455, 0.0009110511944006454, 0.7310585786300049]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import math\n",
        "\n",
        "## Activation Function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + math.exp(-x))\n",
        "\n",
        "## Inputs: (color, stalk, BIAS parameter)\n",
        "inputs = np.array([\n",
        "             [ 1, 10, 1],\n",
        "             [ 2, 25, 1],\n",
        "             [ 3, 22, 1],\n",
        "             [ 4, 20, 1],\n",
        "             [ 4, 27, 1],\n",
        "             [ 5, 23, 1],\n",
        "             [ 6, 1, 1],\n",
        "             [ 7, 8, 1],\n",
        "             [ 8, 1, 1],\n",
        "             [ 9, 18, 1],\n",
        "             [ 10, 8, 1],\n",
        "                                   \n",
        "])\n",
        "\n",
        "## Expected outcomes for each input.\n",
        "expected_outputs = np.array([-1, -1, -1, -1, -1, -1, 1, 1, 1, 1, 1])\n",
        "\n",
        "def perceptron_sgd(inputs, expected_outputs):\n",
        "    weights = np.zeros(len(inputs[0]))\n",
        "    learning_rate = 1\n",
        "    epochs = 100\n",
        "\n",
        "    ## Perceptron weights adjusting  \n",
        "    for t in range(epochs):\n",
        "        for i, x in enumerate(inputs):\n",
        "            if (np.dot(inputs[i], weights) * expected_outputs[i]) <= 0:\n",
        "                weights = weights + learning_rate * inputs[i] * expected_outputs[i]\n",
        "    return weights\n",
        "\n",
        "weights = perceptron_sgd(inputs,expected_outputs)\n",
        "weights = [sigmoid(x) for x in weights]\n",
        "print(weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WquiDC50KEv7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
